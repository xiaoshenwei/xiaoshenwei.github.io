# å¤§æ¨¡å‹åŠ©åŠ›æ—¥å¿—åˆ†æ

<img src="https://raw.githubusercontent.com/xiaoshenwei/xiaoshenwei.github.io/master/assets/images/logai.png" style="zoom:60%;" />

ä»å¤§é‡çš„é”™è¯¯æ—¥å¿—ä¸­è·å–å…³é”®ä¿¡æ¯ï¼Œ å¿«é€Ÿå®šä½é—®é¢˜



# æ¶æ„

![](https://raw.githubusercontent.com/xiaoshenwei/xiaoshenwei.github.io/master/assets/images/2024-08-28-1508.png)

# ä»esä¸­è·å–æ—¥å¿—

 ```python
 import os
 from elasticsearch import Elasticsearch
 
 client = Elasticsearch(
     os.environ["ELASTICSEARCH_HOST"],
     api_key=os.environ["ELASTICSEARCH_API_KEY"],
 )
 
 
 def get_query_body(name: str) -> dict:
     return {
         "sort": [{"@timestamp": {"order": "desc"}}],
         "_source": ["message"],
         "size": 200,
         "query": {
             "bool": {
                 "filter": [
                     {"match": {"message": "error"}},
                     {"range": {"@timestamp": {"gte": "now-10m", "lte": "now"}}},
                     {"term": {"app.keyword": name}},
                 ]
             }
         },
     }
 
 
 def query(name: str):
     return client.search(
         index="logs-*",
         body=get_query_body(name),
     )
 
 
 if __name__ == "__main__":
     print(query("dotcom-grey"))
 
 ```

åªè·å– 10min å†…é”™è¯¯æ—¥å¿—ï¼Œ ç”±äºå¤§æ¨¡å‹token ä¸€èˆ¬éƒ½æœ‰é™åˆ¶ï¼Œ æŒ‰æ—¶é—´ååºï¼Œåªè·å– 200 æ¡æ•°æ®



# Langchain è°ƒç”¨å¤§æ¨¡å‹

```python
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate

llm = init_chat_model(
    "qwen2.5:7b",
    model_provider="ollama",
    base_url="http://ollama-api.k8s-test.uc.host.dxy"
)

prompt_template = """
ä½ æ˜¯ä¸€å Java èµ„æ·±è¿ç»´å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£åˆ†æ Java ç¨‹åºçš„é”™è¯¯æ—¥å¿—ã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†ææ—¥å¿—ï¼š
1. è¯†åˆ«é”™è¯¯ç±»å‹ï¼ˆå¦‚ `NullPointerException`ã€`SQLException`ï¼‰
2. æå–é”™è¯¯æ¶ˆæ¯
3. æ‰¾å‡ºé”™è¯¯å‘ç”Ÿçš„ä»£ç ä½ç½®ï¼ˆç±»ã€æ–¹æ³•ã€è¡Œå·ï¼‰
4. ç»“åˆä¸Šä¸‹æ–‡ï¼Œåˆ†æé—®é¢˜çš„æ ¹å› 

æ—¥å¿—å†…å®¹ï¼š
{context}
"""

prompt = ChatPromptTemplate.from_template(prompt_template)
llm_with_prompt = prompt | llm

logs = """
2023-08-01 12:00:00 ERROR [main] com.example.App - NullPointerException: null
at com.example.App.main(App.java:10)
2023-08-01 12:00:00 ERROR [main] com.example.App - SQLException: SQL syntax error
at com.example.App.main(App.java:20)
"""

result = llm_with_prompt.invoke({"context": logs})

if __name__ == "__main__":
    print(llm.get_num_tokens(logs))
    print(result.content)

```

# æ€è€ƒ

- å¦‚æœæ—¥å¿—é‡å¾ˆå¤§ï¼Œ æ¨¡å‹ æ”¯æŒçš„æœ€å¤§ token æœ‰é™
- å¦‚æœé€»è¾‘å¤æ‚ï¼Œchain å¾ˆå¤æ‚

æœ‰æ²¡æœ‰ç›´æ¥å¯ä»¥å¯åŠ¨ agent æœåŠ¡ï¼Œç”±client å»è°ƒç”¨çš„æ–¹å¼ï¼Ÿ

# LangGraph

| èƒ½åŠ›          | LangChain  | LangGraph    |
| ------------- | ---------- | ------------ |
| çŠ¶æ€ç®¡ç†      | âŒ          | âœ…            |
| åˆ†æ”¯é€»è¾‘      | â›” ç®€å•æ”¯æŒ | âœ… åŸç”Ÿæ”¯æŒ   |
| å¾ªç¯é€»è¾‘      | â›” å¾ˆéš¾å®ç° | âœ… åŸç”Ÿæ”¯æŒ   |
| å¤š Agent åä½œ | âš ï¸ ä¸æ–¹ä¾¿   | âœ… éå¸¸æ–¹ä¾¿   |
| å¤æ‚ç³»ç»Ÿæ„å»º  | âš ï¸ å®¹æ˜“ä¹±   | âœ… ç»“æ„æ¸…æ™°   |
| å¯è§†åŒ–        | ğŸš«          | âœ… æ”¯æŒæµç¨‹å›¾ |

## æ¼”ç¤º

```python
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "qwen2.5:7b",
    model_provider="ollama",
    base_url="http://ollama-api.k8s-test.uc.host.dxy"
)

def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# The first argument is the unique node name
# The second argument is the function or object that will be called whenever
# the node is used.
graph_builder.add_node("chatbot", chatbot)

graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)
graph = graph_builder.compile()

from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
  
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)


while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
```



### TypedDict

æŠŠç±»å‹æç¤ºæ·»åŠ è‡³å­—å…¸çš„ç‰¹æ®Šæ„é€ å™¨ã€‚åœ¨è¿è¡Œæ—¶ï¼Œå®ƒæ˜¯çº¯ [`dict`](https://docs.python.org/zh-cn/3/library/stdtypes.html#dict)ã€‚

```python
from typing import TypedDict, Literal

class User(TypedDict):
    name: str
    age: int
    gender: Literal["ç”·", "å¥³"]
```

### add_message

åœ¨ LangGraph é‡Œï¼Œä¸€ä¸ªèŠ‚ç‚¹å¯èƒ½ä¼šè¢«å¤šä¸ªè¾¹ï¼ˆå¤šæ¡è·¯å¾„ï¼‰å¹¶å‘è°ƒç”¨ï¼Œæœ€ååˆå¹¶æˆä¸€ä¸ªçŠ¶æ€ï¼ˆStateï¼‰ã€‚
 ä¸ºäº†è®©æŸäº›å­—æ®µåœ¨åˆå¹¶æ—¶ **ä¸æ˜¯ç›´æ¥è¦†ç›–ï¼Œè€Œæ˜¯â€œè¿½åŠ â€æˆ–â€œè‡ªå®šä¹‰èšåˆâ€**ï¼Œå°±è¦ç”¨ `Annotated[...]` æŒ‡å®šåˆå¹¶é€»è¾‘ã€‚

```python
from typing import Annotated

from typing_extensions import TypedDict


def add(messages: list[str], new_messages: str) -> list[str]:
    messages.append(new_messages)
    return messages


def replace(messages: list[str], new_messages: str) -> list[str]:
    messages.clear()
    messages.append(new_messages)
    return messages


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add]


class StateManager:
    def __init__(self, state: State):
        self.state = state

    def invoke(self, message: str):
        # ä» State ç±»è·å– messages å­—æ®µçš„æ³¨è§£
        annotation = State.__annotations__["messages"]
        f = annotation.__metadata__[0]
        f(self.state["messages"], message)


if __name__ == "__main__":
    state = State(messages=[])
    state_manager = StateManager(state)
    state_manager.invoke("Hello")
    state_manager.invoke("World")
    state_manager.invoke("ai")
    print(state)
```

## é¡ºåºæ‰§è¡Œ

```python
from langgraph.graph import START, StateGraph
from typing_extensions import TypedDict


class State(TypedDict):
    value_1: str
    value_2: int
    
def step_1(state: State):
    return {"value_1": "a"}


def step_2(state: State):
    current_value_1 = state["value_1"]
    return {"value_1": f"{current_value_1} b"}


def step_3(state: State):
    return {"value_2": 10}

graph_builder = StateGraph(State)

# Add nodes
graph_builder.add_node(step_1)
graph_builder.add_node(step_2)
graph_builder.add_node(step_3)

# Add edges
graph_builder.add_edge(START, "step_1")
graph_builder.add_edge("step_1", "step_2")
graph_builder.add_edge("step_2", "step_3")

graph = graph_builder.compile()

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

graph.invoke({"value_1": "c"})
```

## å¹¶è¡Œæ‰§è¡Œ

å¹¶è¡ŒèŠ‚ç‚¹

```python
import operator
from typing import Annotated, Any

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END


class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]


def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}


def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}


def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}


def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}


builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

graph.invoke({"aggregate": []}, {"configurable": {"thread_id": "foo"}})
```

æ¡ä»¶åˆ†æ”¯

å¦‚æœæ‚¨çš„æ‰‡å‡ºä¸æ˜¯ç¡®å®šçš„ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ add_conditional_edgesã€‚

```python
import operator
from typing import Annotated, Sequence

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END


class State(TypedDict):
    aggregate: Annotated[list, operator.add]
    # Add a key to the state. We will set this key to determine
    # how we branch.
    which: str


def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}


def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}


def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}


def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}


def e(state: State):
    print(f'Adding "E" to {state["aggregate"]}')
    return {"aggregate": ["E"]}


builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_node(e)
builder.add_edge(START, "a")


def route_bc_or_cd(state: State) -> Sequence[str]:
    if state["which"] == "cd":
        return ["c", "d"]
    return ["b", "c"]


intermediates = ["b", "c", "d"]
builder.add_conditional_edges(
    "a",
    route_bc_or_cd,
    intermediates,
)
for node in intermediates:
    builder.add_edge(node, "e")

builder.add_edge("e", END)
graph = builder.compile()

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

graph.invoke({"aggregate": [], "which": "cd"})
```

## MapReduce

Map-reduce æ“ä½œå¯¹äºé«˜æ•ˆçš„ä»»åŠ¡åˆ†è§£å’Œå¹¶è¡Œå¤„ç†è‡³å…³é‡è¦ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠå°†ä»»åŠ¡åˆ†è§£æˆæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶è¡Œå¤„ç†æ¯ä¸ªå­ä»»åŠ¡ï¼Œå¹¶æ±‡æ€»æ‰€æœ‰å·²å®Œæˆå­ä»»åŠ¡çš„ç»“æœã€‚



ç»™å®šç”¨æˆ·çš„ä¸€ä¸ªé€šç”¨ä¸»é¢˜ï¼Œç”Ÿæˆä¸€ç³»åˆ—ç›¸å…³ä¸»é¢˜ï¼Œä¸ºæ¯ä¸ªä¸»é¢˜ç”Ÿæˆä¸€ä¸ªç¬‘è¯ï¼Œå¹¶ä»ç”Ÿæˆçš„ç¬‘è¯åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä½³ç¬‘è¯ã€‚åœ¨è¿™ä¸ªè®¾è®¡æ¨¡å¼ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹å¯èƒ½ç”Ÿæˆä¸€ç³»åˆ—å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œç›¸å…³ä¸»é¢˜ï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›å°†è¿™äº›å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œä¸»é¢˜ï¼‰åº”ç”¨äºå…¶ä»–èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ï¼Œç”Ÿæˆç¬‘è¯ï¼‰ã€‚ç„¶è€Œï¼Œå‡ºç°äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚

- å¯¹è±¡çš„æ•°é‡ï¼ˆä¾‹å¦‚ï¼Œä¸»é¢˜ï¼‰å¯èƒ½åœ¨å¸ƒå±€å›¾ä¹‹å‰æœªçŸ¥
- è¾“å…¥çŠ¶æ€åˆ°ä¸‹æ¸¸èŠ‚ç‚¹çš„åº”è¯¥ä¸åŒï¼ˆæ¯ä¸ªç”Ÿæˆçš„å¯¹è±¡ä¸€ä¸ªï¼‰ã€‚

![image-20250416163553415](/Users/shenweixiao/Library/Application Support/typora-user-images/image-20250416163553415.png)



```python
import operator
from typing import Annotated
from typing_extensions import TypedDict

from langgraph.types import Send
from langgraph.graph import END, StateGraph, START

from pydantic import BaseModel, Field

# æç¤ºæ¨¡æ¿å®šä¹‰ï¼ˆä¸­æ–‡ï¼‰
subjects_prompt = """ç”Ÿæˆ 2 åˆ° 5 ä¸ªä¸ä»¥ä¸‹ä¸»é¢˜ç›¸å…³çš„ç¤ºä¾‹ï¼Œä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼š{topic}"""
joke_prompt = """ç”Ÿæˆä¸€ä¸ªå…³äº {subject} çš„ç¬‘è¯"""
best_joke_prompt = """ä»¥ä¸‹æ˜¯ä¸€äº›å…³äº {topic} çš„ç¬‘è¯ã€‚è¯·é€‰æ‹©å…¶ä¸­æœ€å¥½çš„ä¸€ä¸ªï¼è¿”å›æœ€å¥½çš„ç¬‘è¯çš„ IDï¼ˆä» 0 å¼€å§‹è®¡æ•°ï¼‰ã€‚

{jokes}"""

# å®šä¹‰è¿”å›çš„ subjects æ•°æ®ç»“æ„
class Subjects(BaseModel):
    subjects: list[str]

# å®šä¹‰è¿”å›çš„ joke æ•°æ®ç»“æ„
class Joke(BaseModel):
    joke: str

# å®šä¹‰è¿”å›çš„ best_joke æ•°æ®ç»“æ„ï¼ŒID ä» 0 å¼€å§‹
class BestJoke(BaseModel):
    id: int = Field(description="Index of the best joke, starting with 0", ge=0)

# åˆå§‹åŒ– Claude æ¨¡å‹
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "qwen2.5:7b",
    model_provider="ollama",
    base_url="http://ollama-api.k8s-test.uc.host.dxy"
)

# å®šä¹‰ä¸»æµç¨‹çŠ¶æ€ç»“æ„
class OverallState(TypedDict):
    topic: str  # ä¸»é¢˜ï¼Œç”±ç”¨æˆ·æä¾›
    subjects: list  # ç”Ÿæˆçš„å­ä¸»é¢˜
    jokes: Annotated[list, operator.add]  # åˆå¹¶æ‰€æœ‰å­èŠ‚ç‚¹ç”Ÿæˆçš„ç¬‘è¯
    best_selected_joke: str  # æœ€ä½³ç¬‘è¯

# å®šä¹‰ joke èŠ‚ç‚¹ä½¿ç”¨çš„çŠ¶æ€ç»“æ„
class JokeState(TypedDict):
    subject: str  # å•ä¸ªå­ä¸»é¢˜

# èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æä¾›çš„ topic ç”Ÿæˆå­ä¸»é¢˜åˆ—è¡¨
def generate_topics(state: OverallState):
    prompt = subjects_prompt.format(topic=state["topic"])
    response = model.with_structured_output(Subjects).invoke(prompt)
    return {"subjects": response.subjects}

# èŠ‚ç‚¹ï¼šæ ¹æ®å­ä¸»é¢˜ç”Ÿæˆç¬‘è¯
def generate_joke(state: JokeState):
    prompt = joke_prompt.format(subject=state["subject"])
    response = model.with_structured_output(Joke).invoke(prompt)
    return {"jokes": [response.joke]}  # è¿”å›åˆ—è¡¨æ–¹ä¾¿èšåˆ

# è¾¹ï¼šå°†å¤šä¸ª subject åˆ†å‘åˆ° joke èŠ‚ç‚¹
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

# èŠ‚ç‚¹ï¼šä»å¤šä¸ªç¬‘è¯ä¸­é€‰å‡ºæœ€å¥½çš„ä¸€ä¸ª
def best_joke(state: OverallState):
    jokes = "\n\n".join(state["jokes"])  # æ‹¼æ¥æ‰€æœ‰ç¬‘è¯
    prompt = best_joke_prompt.format(topic=state["topic"], jokes=jokes)
    response = model.with_structured_output(BestJoke).invoke(prompt)
    return {"best_selected_joke": state["jokes"][response.id]}  # å–å‡ºæœ€ä¼˜ç¬‘è¯

# æ„å»ºæµç¨‹å›¾
graph = StateGraph(OverallState)

graph.add_node("generate_topics", generate_topics)  # æ·»åŠ ç”Ÿæˆå­ä¸»é¢˜èŠ‚ç‚¹
graph.add_node("generate_joke", generate_joke)  # æ·»åŠ ç”Ÿæˆç¬‘è¯èŠ‚ç‚¹
graph.add_node("best_joke", best_joke)  # æ·»åŠ è¯„é€‰ç¬‘è¯èŠ‚ç‚¹

graph.add_edge(START, "generate_topics")  # èµ·ç‚¹ -> ç”Ÿæˆå­ä¸»é¢˜
graph.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])  # å­ä¸»é¢˜ -> å¹¶å‘ç”Ÿæˆç¬‘è¯
graph.add_edge("generate_joke", "best_joke")  # æ‰€æœ‰ç¬‘è¯ç”Ÿæˆå®Œ -> è¯„é€‰

graph.add_edge("best_joke", END)  # æœ€ç»ˆç»“æŸ

# ç¼–è¯‘ä¸º LangGraph App
app = graph.compile()


from IPython.display import Image, display

display(Image(app.get_graph().draw_mermaid_png()))

# Call the graph: here we call it to generate a list of jokes
for s in app.stream({"topic": "animals"}):
    print(s)

print("DONE")
```

# éƒ¨ç½²agent-server

uv + langgraph-cli

ç›®å½•ç»“æ„, ä¸¥æ ¼ä¸€ç›´ï¼Œå¦åˆ™æ— æ³•è¯†åˆ«

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ langgraph.json
â”œâ”€â”€ main.py
â”œâ”€â”€ my_agent
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ agent.py
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ es.py
â”‚Â Â      â”œâ”€â”€ prompts.py
â”‚Â Â      â”œâ”€â”€ state.py
â”‚Â Â      â””â”€â”€ utils.py
â”œâ”€â”€ pyproject.toml
â””â”€â”€ uv.lock

```

## langgraph.json

```json
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./my_agent/agent.py:graph"
  },
  "env": ".env"
}
```

## pyproject.toml

```toml
[project]
name = "friday-ai"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "elasticsearch>=8.17.2",
    "langchain>=0.3.22",
    "langchain-ollama>=0.3.0",
    "langgraph==0.3.21",
    "transformers>=4.50.3",
]

```

## .env

ç¼–è¾‘æ‚¨çš„.envæ–‡ä»¶



## dockerfile

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆ Dockerfile

```bash
langgraph dockerfile Dockerfile 
```

# éƒ¨ç½²agent-client

è´Ÿè´£æä¾›æ¥å£å¯¹å¤–æä¾›æœåŠ¡

```python
import json

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from langgraph_sdk import get_client

app = FastAPI()


# If you're using a remote server, initialize the client with `get_client(url=REMOTE_URL)`
client = get_client(url="http://0.0.0.0:8123")


@app.get("/api/ai/log/diagnosis/{appname}")
async def get_diagnosis(appname: str):
    # List all assistants
    assistants = await client.assistants.search()
    # We auto-create an assistant for each graph you register in config.
    agent = assistants[0]

    thread = await client.threads.create()

    async def run_stream():
        input_data = {"appname": appname}
        async for chunk in client.runs.stream(thread['thread_id'], agent['assistant_id'],
                                              input=input_data,
                                              stream_mode="values"):
            if chunk.data:
                yield f"""data: {json.dumps(chunk.data)}\n\n"""

    return StreamingResponse(run_stream(), media_type="text/event-stream")

```

``` bash
 uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```



## NGINX è½¬å‘

```python
  location /api/ai/ {
          proxy_pass http://0.0.0.0:8000;
          proxy_set_header Host $host;
          proxy_buffering off;
          proxy_cache off;
  }
```

