---
toc: true
title: "RAG扫盲?"
categories:
  - llm
tags:
  - rag
  - ollma
---

# 挑战开始

准备条件:
- 127.0.0.1:11434  部署本地ollma
- 127.0.0.1:6008 部署本地m3e

## 第一关： 与AI 接头

```python
from openai import OpenAI

client = OpenAI(
    base_url = 'http://127.0.0.1:11434/v1',
    api_key='ollama', # required, but unused
)

response = client.chat.completions.create(
  model="qwen:7b",
  messages=[
    {"role": "system", "content": "你是一名python开发工程师, 名字叫小五"},
    {"role": "user", "content": "介绍一下你自己"}
  ]
)
print(response.choices[0].message.content)

curl http://127.0.0.1:11434/api/embed -d '{
  "model": "mxbai-embed-large:latest",
  "input": "Why is the sky blue?"
}'
```

## 第二关: 与AI 聊天

```python
from openai import OpenAI

client = OpenAI(
    base_url = 'http://127.0.0.1:11434/v1',
    api_key='ollama', # required, but unused
)

history = [
    {"role": "system", "content": "你是一名python开发工程师, 名字叫小五"}
]

def ask():
    response = client.chat.completions.create(
        model="qwen:7b",
        messages=history,
    )
    return response

def main():
    while 1:
        user_input = input(">>> ")
        if user_input.lower() in ["bye", "goodbye", "exit"]:
            print("Goodbye!") 
            break
        history.append({"role": "user", "content": user_input})
        response = ask()
        print(response.choices[0].message.content)
        print("tokens: ", response.usage.total_tokens)
        history.append({"role": "assistant", "content": response.choices[0].message.content})

print("你好，我是一个聊天机器人，请你提出你的问题吧?")
main()
```

- 随着聊天次数的增加，每次对话token 数量直线上升, 使用 AI 的 Summary 能力解决。

## 第三关: 让 AI 帮你工作

领导让我做一个 商品推荐 的功能。
```
1. 数据收集与预处理
2. 选择推荐算法
  基于内容的推荐
    原理：根据用户的历史行为和商品的特征，推荐与用户过去喜欢的商品相似的商品。
    算法：TF-IDF、余弦相似度、KNN等
  
3. 模型训练与评估
  划分数据集：将数据集划分为训练集和测试集。
  训练模型：使用训练集数据训练推荐模型。
  评估模型：使用测试集数据评估模型的性能，计算评估指标如MSE、准确率等。

4. 模型优化与部署
  模型优化：调整模型超参数，选择最佳模型。
  模型部署：将模型部署到生产环境中，实现实时推荐。
```

让AI 生成 商品列表
```
prompt: 
1. 使用中文生成50条京东商品数据, 
2. 每条数据包含 商品名称Name, 商品详细的描述Desc 不少于20字 ,
3. 商品要包含3C，服饰，食品,
4. 不能出现名字相同的商品 直接以csv 格式输出
```

计算 

```python
import pandas as pd
import requests
from scipy.spatial.distance import cosine

df = pd.read_csv("product.csv")

# 显示前几行数据
# print(df.head())

books_list = []

book_embeddings = []

def get_embedding(message):
    response = requests.post("http://127.0.0.1:6008/v1/embeddings", json={
        "model": "m3e",
        "input": [message]
    }, headers={"Authorization": "Bearer sk-aaabbbcccdddeeefffggghhhiiijjjkkk"})
    json_data = response.json()
    return json_data.get("data")[0].get("embedding")

# test
# print(get_embedding("智能手机X10"))

for index, row in df.iterrows():
    str_row = f"{row['Name']} {row['Desc']}"
    book_embeddings.append(get_embedding(str_row))
    print(index, "向量计算完成")


df["embedding"] = book_embeddings
df.to_parquet("product.parquet", index=False)

```

搜索

```python
import pandas as pd
import requests
from scipy.spatial.distance import cosine

def get_embedding(message):
    response = requests.post("http://127.0.0.1:6008/v1/embeddings", json={
        "model": "m3e",
        "input": [message]
    }, headers={"Authorization": "Bearer sk-aaabbbcccdddeeefffggghhhiiijjjkkk"})
    json_data = response.json()
    return json_data.get("data")[0].get("embedding")

def p_search(query, top=3):
    df = pd.read_parquet("product.parquet", engine='pyarrow')
    quey_embeddings = get_embedding(query)
    # scipy.spatial.distance.cosine 返回的其实是 余弦距离，而不是相似度。
    # 余弦距离越小表示越相似 而不是越大越相似
    # 因此，需要调整为按照升序排列，而不是降序排列。
    df["similarity"] = df.embedding.apply(lambda x: cosine(x, quey_embeddings))
    results = df.sort_values("similarity", ascending=True).head(top)
    return results

results = p_search("饿了")

# # 打印结果
for i, row in results.iterrows():
    print(row['Name'], row['Desc'])
```

向量的存储 使用 faiss-gpu

```python
import pandas as pd
import requests
import faiss
import numpy as np
from scipy.spatial.distance import cosine

df = pd.read_csv("product.csv")

# 显示前几行数据
# print(df.head())

books_list = []

book_embeddings = []

def get_embedding(message):
    response = requests.post("http://127.0.0.1:6008/v1/embeddings", json={
        "model": "m3e",
        "input": [message]
    }, headers={"Authorization": "Bearer sk-aaabbbcccdddeeefffggghhhiiijjjkkk"})
    json_data = response.json()
    return json_data.get("data")[0].get("embedding")


for index, row in df.iterrows():
    str_row = f"{row['Name']} {row['Desc']}"
    book_embeddings.append(get_embedding(str_row))
    print(index, "向量计算完成")


def load_embeddings_to_faiss(df): 
    embeddings = np.array(df['embedding'].tolist()).astype('float32')
    index = faiss.IndexFlatL2(embeddings.shape[1]) 
    index.add(embeddings) 
    return index

df["embedding"] = book_embeddings

index = load_embeddings_to_faiss(df)

def search_index(index, df, query, k=5):
    query_vector = np.array(get_embedding(query)).reshape(1, -1).astype('float32')
    distances, indexes = index.search(query_vector, k)
    results = []
    for i in range(len(indexes)):
        product_names = df.iloc[indexes[i]]['Name'].values.tolist()
        results.append((distances[i], product_names))
    return results

products = search_index(index, df, "饿了", k=3)

# # 打印结果
for i, row in products:
    print(i, row)
```
## 第四关: 让 AI 帮你读书

```
prompt: 
1. 帮我写一本类似斗破苍穹的玄幻小说，1000字左右
2. 男主叫萧火, 女主叫熏儿, boos叫 魂旦
3. 结局悲惨一点
```



```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama

documents = SimpleDirectoryReader("data").load_data()

# bge-base embedding model
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

# ollama
Settings.llm = Ollama(base_url="http://127.0.0.1:11434", model="llama3:latest", request_timeout=360.0)

index = VectorStoreIndex.from_documents(
    documents,
)
query_engine = index.as_query_engine()
response = query_engine.query("中文回答我 熏儿最后死了吗?")
print(response)

```

#  Embedding 

Embedding 嵌入是指将文本、图像、音频、视频等形式的信息映射为高维空间中的密集向量表示。这些向量在语义空间中起到坐标的作用，捕捉对象之间的语义关系和隐含的意义。通过在向量空间中进行计算（例如余弦相似度），可以量化和衡量这些对象之间的语义相似性。 ![](https://static001.geekbang.org/resource/image/60/09/60d4d55a2c7d16d42e255e19038eb409.jpg?wh=1920x1080)

Embedding Model 嵌入模型

自 2013 年以来，word2vec、GloVe、fastText 等嵌入模型通过分析大量文本数据，学习得出单词的嵌入向量。近年来，随着 transformer 模型的突破，嵌入技术以惊人的速度发展。BERT、RoBERTa、ELECTRA 等模型将词嵌入推进到上下文敏感的阶段。这些模型在为文本中的每个单词生成嵌入时，会充分考虑其上下文环境，因此同一个单词在不同语境下的嵌入向量可以有所不同，从而大大提升了模型理解复杂语言结构的能力。

# RAG

![](https://static001.geekbang.org/resource/image/a7/e2/a77fd6ff269ac17186672931040dc0e2.png?wh=1920x832)

大模型功能强大，但它当前仍存在幻觉、知识时效性、领域知识不足及数据安全问题的局限性。RAG（Retrieval Augmented Generation，检索增强生成）技术正是在这样的背景下应运而生，成为了当前大模型应用的重要技术方向，文档问答类 LLM RAG 应用也被认为是 AI 2.0 时代最早落地的应用类型之一。

RAG 技术使开发者能够在无需为每个特定任务重新训练或微调大模型的情况下，通过连接外部知识库和文档，为模型注入额外的非参数化知识，从而显著提升其在专业领域的能力和回答精度。RAG 技术赋予大模型在各行各业和各种垂直场景中巨大的效率和体验价值，有望成为最快涌现的“杀手级应用”，已成为大模型应用落地最热门的方向之一。

选择 RAG 而不是直接将所有知识库数据交给大模型处理，主要是因为模型能够处理的 token 数有限，输入过多 token 会增加成本。更重要的是，提供少量相关的关键信息能够带来更优质的回答。 

![](https://static001.geekbang.org/resource/image/5f/0a/5f10a52ebc4d00ed4bc3ceyy756c6d0a.png?wh=2048x1210 )

## RAG 技术应用场景

RAG 技术凭借其将检索与生成相结合的优势，可广泛应用于多个领域和场景，满足了在大模型应用中实时性、高准确性和领域专有知识获取的需求。在企业或领域知识管理与问答系统中，RAG 能够实时从企业或领域的私有知识库中检索相关信息，确保生成的回答不仅准确且符合企业内部的最新动态，解决了大模型在处理特定领域知识时的局限性。

## RAG 标准技术流程

 ![](https://static001.geekbang.org/resource/image/7b/dc/7bc529003e05a3ab0561204230a83bdc.png?wh=1898x1008)

RAG 标准流程由索引（Indexing）、检索（Retriever）和生成（Generation）三个核心阶段组成

1. 索引阶段，通过处理多种来源多种格式的文档提取其中文本，将其切分为标准长度的文本块（chunk），并进行嵌入向量化（embedding），向量存储在向量数据库（vector database）中。
2. 检索阶段，用户输入的查询（query）被转化为向量表示，通过相似度匹配从向量数据库中检索出最相关的文本块。
3. 最后生成阶段，检索到的相关文本与原始查询共同构成提示词（Prompt），输入大语言模型（LLM），生成精确且具备上下文关联的回答。

索引是 RAG 系统的基础环节，包含四个关键步骤。

1. 首先，将各类数据源及其格式（如书籍、教材、领域数据、企业文档等，txt、markdown、doc、ppt、excel、pdf、html、json 等格式）统一解析为纯文本格式。
2. 接着，根据文本的语义或文档结构，将文档分割为小而语义完整的文本块（chunks），确保系统能够高效检索和利用这些块中包含的信息。
3. 然后，使用文本嵌入模型（embedding model），将这些文本块向量化，生成高维稠密向量，转换为计算机可理解的语义表示。
4. 最后，将这些向量存储在向量数据库 (vector database) 中，并构建索引，完成知识库的构建。这一流程成功将外部文档转化为可检索的向量，支撑后续的检索和生成环节。

检索是连接用户查询与知识库的核心环节。

1. 首先，用户输入的问题通过同样的文本嵌入模型转换为向量表示，将查询映射到与知识库内容相同的向量空间中。
2. 通过相似度度量方法，检索模块从向量数据库中筛选出与查询最相关的前 K 个文本块，这些文本块将作为生成阶段输入的一部分。
3. 通过相似性搜索，检索模块有效获取了与用户查询切实相关的外部知识，为生成阶段提供了精确且有意义的上下文支持。

生成是 RAG 流程中的最终环节，将检索到的相关文本块与用户的原始查询整合为增强提示词（Prompt），并输入到大语言模型（LLM）中。LLM 基于这些输入生成最终的回答，确保生成内容既符合用户的查询意图，又充分利用了检索到的上下文信息，使得回答更加准确和相关，充分使用到知识库中的知识。通过这一过程，RAG 实现了具备领域知识和私有信息的精确内容生成。
