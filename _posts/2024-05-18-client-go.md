---
toc: true
title: "client-go 使用"
categories:
  - devops
tags:
  - k8s
---

# GVR/GVK

## 简介

Kubernetes API 是通过HTTP协议以RESTful 的形式提供的，同时支持JSON 和Protobuf 的数据格式.

在Kubernetes API中, 我们一般使用GVR 或者 GVK 来区分特定的资源, 即根据不同的分组，版本以及资源，进行url的定义。

Kubernetes的API 分组，分为`无组名资源组` 和 `有组名资源组`， `无组名资源组` 也称为核心资源组, Core Group

有组名的资源组

```
            /apis/apps/v1/deployments
                   |   |     |_ _ _ _ _ Resource
          Group _ _|   |
                       |_ _ Version
                 
```

无组名分组

```
						/api/v1/pods
						     |   |_ _ _ _ Resource
						     |_ _ Version
```



GVR / GVK 定义

- G (Group) : 资源组，包含一组资源操作的集合
- V (Version)： 资源版本, 用于区分不同API 的稳定程度以及兼容性
- R (Resource): 资源信息，用于区分不同资源的API
- K (Kind): 资源对象类型，每个资源都需要Kind 来区分他自身代表的资源类型

通过GVR 组合 RESTful API 请求路径

```
GET /apis/apps/v1/namespace/{namespace}/deployments/{deployment_name}
```

GVK，相反， 通过GVK信息则可以获取要读取的资源的GVR. 这种GVK, GVR 的映射叫做RESTMapper.

RESTMapper 的主要作用是在ListerWatcher时, 根据Schema 定义的类型GVK 解析出GVR, 向APIServer 发起HTTP资源请求。

> 简单通俗点来讲：`Kind` 就是我们的**资源种类**，如 `Pod`、`Deployment` 等，`Resource` 是 `Kind` 资源种类的**资源子类**，如：`pods`、`services`、`deployments`
>
> 更通俗点: Kind 就是Class, Resource 就是Objects

# client-go

Client-GO 是负责与Kubernetes APIServer 服务交互的客户端库，利用Client-Go与Kubernetes APIServer 进行交互访问，以此来对 Kubernetes 的各类资源对象进行管理操作.

Client-Go 提供了4种与Kubernetes APIServer 交互的客户端对象, 分别是：

- RESTClient: 最基础的客户端，主要对HTTP 请求进行封装，支持JSON 和Protobuf格式的数据。
- ClientSet: 预定义的客户端，负责操作Kubernetes 内置的资源对象， 例如: Pod, Svc
- DiscoveryClient: 发现客户端，负责发现APIServer 支持的资源组，资源版本，资源信息
- DynamicClient: 动态客户端，可以对任意的Kubernetes 资源对象进行通用的操作，包括CRD

## Client客户端

### DynamicClient

使用ClientSet 的时候，程序会将所用的版本与类型紧密耦合，而DynamicClient 使用嵌套的map[string]interface{} 结构存储 Kubernetes APIServer 的返回值，使用反射机制，`在运行的时候进行数据绑定`。 这种方式更加灵活，但是无法获取强数据类型的检查和验证。

此外，学些DynamicClient 之前， 需要了解两个只是点 `Object.runtime`接口和 `Unstructured` 结构体

- Object.runtime: Kubernetes 中所有的资源对象，都实现了这个接口，其中包含，DeepCopyObject 和 GetObjectKind 方法，分别用于对象深拷贝和获取对象的具体资源类型。
- Unstructured：包括map[string]interface{} 类型字段，在处理无法阈值结构的数据时，将数据值绑定到interface{}中，待运行时利用反射判断。

### DiscoveryClient

前面的客户端对象，都是针对资源独享管理的， 而DiscoveryClient 是针对GVR的，用户查看当前Kubernetes 集群支持哪些资源组，资源版本，资源信息。

## 代码

client.go 创建各个客户端的快捷封装

```go
package client

import (
	"k8s.io/client-go/discovery"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
)

const KubeConfigPath = "/Users/shenweixiao/.kube/config"

type Client struct {
	Config *rest.Config
}

func NewClient() *Client {
	config, err := clientcmd.BuildConfigFromFlags("", KubeConfigPath)
	if err != nil {
		panic(err)
	}
	// 禁用证书校验
	config.TLSClientConfig.Insecure = true
	return &Client{
		Config: config,
	}
}

func (c *Client) InitRestClient() *rest.RESTClient {
	restClient, err := rest.RESTClientFor(c.Config)
	if err != nil {
		panic(err)
	}
	return restClient
}

func (c *Client) InitClientSet() *kubernetes.Clientset {
	clientSet, err := kubernetes.NewForConfig(c.Config)
	if err != nil {
		panic(err)
	}
	return clientSet
}

func (c *Client) InitDynamicClient() dynamic.Interface {
	dynamicClient, err := dynamic.NewForConfig(c.Config)
	if err != nil {
		panic(err)
	}
	return dynamicClient
}

func (c *Client) InitDiscoveryClient() *discovery.DiscoveryClient {
	discoveryClient, err := discovery.NewDiscoveryClientForConfig(c.Config)
	if err != nil {
		panic(err)
	}
	return discoveryClient
}

```

client_test.go 各client的用法

```go
package client

import (
	"context"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes/scheme"
	"testing"
)

// TestClient_InitRestClient
// 使用restClient 获取所有Pod
func TestClient_InitRestClient(t *testing.T) {
	c := NewClient()
	c.Config.APIPath = "/api"
	c.Config.GroupVersion = &v1.SchemeGroupVersion
	c.Config.NegotiatedSerializer = scheme.Codecs

	restClient := c.InitRestClient()
	result := &v1.PodList{}
	err := restClient.
		Get().
		Namespace("default").
		Resource("pods").
		VersionedParams(&metav1.ListOptions{}, scheme.ParameterCodec).
		Do(context.Background()).
		Into(result)
	if err != nil {
		t.Error(err)
	}
	for _, pod := range result.Items {
		t.Log(pod.Name)
	}
}

func TestClient_InitClientSet(t *testing.T) {
	clientSet := NewClient().InitClientSet()
	pods, err := clientSet.CoreV1().Pods("default").List(context.Background(), metav1.ListOptions{})
	if err != nil {
		t.Error(err)
	}
	for i, item := range pods.Items {
		t.Log(i, item.Name)
	}
}

func TestClient_InitDynamicClient(t *testing.T) {
	dynamicClient := NewClient().InitDynamicClient()
	gvr := schema.GroupVersionResource{
		Group:    "",
		Version:  "v1",
		Resource: "pods",
	}

	unStructData, err := dynamicClient.
		Resource(gvr).Namespace("default").
		List(context.Background(), metav1.ListOptions{})
	if err != nil {
		t.Error(err)
	}

	podList := &v1.PodList{}
	err = runtime.DefaultUnstructuredConverter.FromUnstructured(unStructData.UnstructuredContent(), podList)
	if err != nil {
		t.Error(err)
	}
	for _, item := range podList.Items {
		t.Log(item.Name)
	}
}

func TestClient_InitDiscoveryClient(t *testing.T) {
	discoveryClient := NewClient().InitDiscoveryClient()
	_, apiResourceList, err := discoveryClient.ServerGroupsAndResources()
	if err != nil {
		t.Error(err)
	}
	for _, item := range apiResourceList {
		gv, e := schema.ParseGroupVersion(item.GroupVersion)
		if e != nil {
			t.Error(e)
		}
		for _, resource := range item.APIResources {
			t.Log(gv.Group, gv.Version, resource.Name, resource.Kind)
		}
	}
}

// GVR缓存到本地文件
func TestClient_InitDiscoveryClient_CacheGVR(t *testing.T) {
	discoveryClient := NewClient()
	cachedDiscoveryClient, err := disk.NewCachedDiscoveryClientForConfig(
		discoveryClient.Config,
		"./cache/discovery", "./cache/http", time.Minute)
	if err != nil {
		return
	}
	// 1. 从缓存中获取GVR数据, 有则直接返回, 没有则请求APIServer
	// 2. 请求APIServer
	// 3. 将获取到的gvr数据缓存到本地文件
	_, apiResourceList, err := cachedDiscoveryClient.ServerGroupsAndResources()
	for _, item := range apiResourceList {
		gv, e := schema.ParseGroupVersion(item.GroupVersion)
		if e != nil {
			t.Error(e)
		}
		for _, resource := range item.APIResources {
			t.Log(gv.Group, gv.Version, resource.Name, resource.Kind)
		}
	}

}

```

# Informer

![](https://raw.githubusercontent.com/xiaoshenwei/xiaoshenwei.github.io/master/assets/images/client-go-controller-interaction.jpeg)

Informer 负责与Kubernetes APIServer 进行Watch操作，Watch的资源，可以是Kubernetes 内置资源，也可以是CRD.

Informer 是一个带有本地缓存以及索引机制的核心工具包, 当请求为查询操作时，会优先从本地缓存内去查找数据， 而创建，更新，删除， 这类的操作，则会根据事件通知写入到队列DeltaFIFO中，同时对应的事件处理过后，更新本地缓存，是本地缓存和ETCD的数据保持一致。

Informer 抽象出来的这个缓存层，将查询压力接受了下来，这样就不必每次都去调用APIServer 的接口， 减轻了APIServer d的数据交互压力。

Informer 由以下组件组成:

- Reflector: 使用List-Watch 来保证本地缓存数据的准确定，顺序性，一致性。List 对应资源的全量列表数据, Watch 负责变化部分的数据，Watch 指定的Kubernetes 资源一旦发生变化，就会触发变更事件，比如Added, Updated 和 Deleted 事件，并将资源对象的变化事件存放在本地队列DeltaFIFO中。
- DeltaFIFO：是一个增量队列， 记录了资源变化的过程。Reflector 相当于队列的生产者。 这个组件包含两部分, `FIFO` 就是一个队列，拥有队列的基本方法，例如ADD, UPDATE,DELETE, LIST, POP等。Delta 是一个资源对象存储， 保存存储对象的消费类型，比如Added, Updated, Deleted 等。
- Indexed: 用来存储资源对象并自带索引功能的本地存储。 Reflector 从DeltaFIFO 中讲消费出来的资源对象存储到Indexer, Indexer 与ETCD 中的数据保持一致。 从而client-go 可以本地读取，减少Kubernetes APIServer的数据交互压力。

## List-Watch

> List-Watch 机制是Kubernetes 中的异步消息通知机制，通过他可以有效的保证消息的实时性，顺序性，可靠性

```bash
# master
kubectl proxy
curl http://127.0.0.1:8001/api/v1/namespaces/default/pods
curl http://127.0.0.1:8001/api/v1/namespaces/default/pods/?watch=true
```



List-Watch 分为两部分

- List 负责调用资源对应Kubernetes APIServer 的RESTful API 获取全局数据列表，并同步到本地缓存中。
- Watch 负责监听资源的变化，并调用相应事件的处理函数进行处理，同时更新本地缓存，使本地缓存与ETCD中数据，保持一致。

```go
package informer

import (
	"context"
	selfClient "k8s-demo/client"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"testing"
)

func TestInformer_WatchUsage(t *testing.T) {
	clientSet := selfClient.NewClient().InitClientSet()
	w, err := clientSet.AppsV1().Deployments("default").
		Watch(context.TODO(), metav1.ListOptions{})
	if err != nil {
		t.Error(err)
	}
	for {
		select {
		case event, _ := <-w.ResultChan():
			// Type: 表示时间的类型
			// Object: 表示事件对象
			t.Log(event.Type, event.Object)
		}
	}
}

```

## Reflector

Reflector 核心的部分就是List-Watch

核心逻辑分为三部分:

- List： 调用List 方法获取资源全部列表数据， 转换为资源对象列表，然后保存到本地缓存中
- 定时同步：定时器定时触发同步机制，定时更新缓存数据，在Reflector的结构体对象中， 是可以配置定时同步周期的。
- Watch：监听资源的变化，并调用对应的事件处理函数

## DeltaFIFO

> DeltaFIFO 是一个增量的本地队列，记录了资源对象的变化过程。

他的生产者就是Reflector 组件，将监听的对象同步到DeltaFIFO 中。

`FIFO` 是一个先入先出的本地队列，`Delta` 则是资源对象的变化，例如增加，删除，修改

Delta 有两个属性，分别是Type 和 Object

- Type: 表示这个事件的类型，比如Added 元素增加，Updated 表示更新
- Object: 是一个interface类型， 表示一个具体Kubernetes资源对象，例如：Pod,Service

```go
type Delta struct {
	Type   DeltaType
	Object interface{}
}

// DeltaType is the type of a change (addition, deletion, etc)
type DeltaType string

// Change type definition
const (
	Added   DeltaType = "Added"
	Updated DeltaType = "Updated"
	Deleted DeltaType = "Deleted"
	// Replaced is emitted when we encountered watch errors and had to do a
	// relist. We don't know if the replaced object has changed.
	//
	// NOTE: Previous versions of DeltaFIFO would use Sync for Replace events
	// as well. Hence, Replaced is only emitted when the option
	// EmitDeltaTypeReplaced is true.
	Replaced DeltaType = "Replaced"
	// Sync is for synthetic events during a periodic resync.
	Sync DeltaType = "Sync"
)
```

## Indexer

> 本身就是一个存储，同时在存储的基础上拓展了索引的功能

深入理解Indexer 之前，我们还需要知道Indexer 中几个非常重要的概念:

- IndexFunc: 索引器函数，用于计算一个资源对象的索引值列表，可以根据需求定义其他的，比如根据Label标签，Annotation 等属性来生成索引值列表。
- Index: 存储数据， 要查找某个命名空间下的Pod, 就要让Pod 按照命名空间进行索引，对应的Index 类型就是 `map[namespace]sets.pod`
- Indexers: 存储索引器， key 为索引器名称，value 为索引器的实现函数，例如：`map[namespace]MetaNamespaceIndexFunc`
- Indices: 存储缓存器，key 为索引器名称，value 为缓存的数据,例如: `map[namespace]map[namespace]sets.pod`

可通过以下代码和数据格式加深理解

```go
package informer

import (
	"fmt"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/tools/cache"
	"testing"
)

// NamespaceIndexFunc 根据namespace索引
func NamespaceIndexFunc(obj interface{}) (result []string, err error) {
	pod, ok := obj.(*v1.Pod)
	if !ok {
		return nil, fmt.Errorf("obj is not a pod")
	}
	result = append(result, pod.Namespace)
	return
}

// NodeNameIndexFunc 根据nodeName索引
func NodeNameIndexFunc(obj interface{}) (result []string, err error) {
	pod, ok := obj.(*v1.Pod)
	if !ok {
		return nil, fmt.Errorf("obj is not a pod")
	}
	result = append(result, pod.Spec.NodeName)
	return
}

func TestNewIndexer(t *testing.T) {
	// 1. 注册已经定义的两个索引函数
	index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{
		"namespace": NamespaceIndexFunc,
		"nodeName":  NodeNameIndexFunc,
	})
	// 2. 模拟数据
	pod1 := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "pod1",
			Namespace: "default",
		},
		Spec: v1.PodSpec{
			NodeName: "node1",
		},
	}
	pod2 := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "pod2",
			Namespace: "default",
		},
		Spec: v1.PodSpec{
			NodeName: "node2",
		},
	}
	pod3 := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "pod3",
			Namespace: "kube-system",
		},
		Spec: v1.PodSpec{
			NodeName: "node3",
		},
	}
	
	// 3. 将数据添加到索引器中
	index.Add(pod1)
	index.Add(pod2)
	index.Add(pod3)

	// 4. 通过索引器函数查询数据
	fmt.Println("all pods index by namespace")
	pods, err := index.ByIndex("namespace", "default")
	if err != nil {
		t.Fatalf("failed to get pods by namespace: %v", err)
	}
	for _, pod := range pods {
		t.Logf("pod: %v", pod.(*v1.Pod).Name)
	}

	fmt.Println("all pods index by nodeName")
	pods, err = index.ByIndex("nodeName", "node2")
	if err != nil {
		t.Fatalf("failed to get pods by nodeName: %v", err)
	}
	for _, pod := range pods {
		t.Logf("pod: %v", pod.(*v1.Pod).Name)
	}
}
```

```json
// index 数据格式
{
	 "default": ["pod1", "pod2"],
	 "kube-system": ["pod3"],
	 "node1": ["pod1"],
	 "node2": ["pod2"],
	 "node3": ["pod3"],
}
// indexers 数据格式
{
	 "namespace": NamespaceIndexFunc,
	 "nodeName":  NodeNameIndexFunc,
}
// Indices 数据结构
{
	"namespace": {
		"default": ["pod1", "pod2"],
		"kube-system": ["pod3"],
	},
	"nodeName": {
		"node1": ["pod1"],
		"node2": ["pod2"],
		"node3": ["pod3"],
	}
}
```

`ThreadSafeMap`: 这是一个并发安全的存储，Indexer 就是在其基础上进行封装的，实现了索引相关的功能

```go
// threadSafeMap implements ThreadSafeStore
type threadSafeMap struct {
	lock  sync.RWMutex
	items map[string]interface{}

	// index implements the indexing functionality
	index *storeIndex
}
```

# SharedInformer

Kubernetes 中会运行很多的控制器，有很多的资源需要管理, 难免会出现一个资源受到多个控制器管理。

为了应对这种场景，可以通过 `SharedInformer` 来创建一份供多个控制器共享的缓存

SharedInformer 一般使用SharedInformerFactory 来管理控制器需要的资源对象的Informer 实例，使用map的结构进行存储。

`SharedIndexInformer` 在其基础上扩展了添加和获取Indexers的能力

## SharedInformerFactory

`SharedInformerFactory` 为所有已知的GVR 提供共享的informer.

```go
type SharedInformerFactory interface {
	internalinterfaces.SharedInformerFactory
	Start(stopCh <-chan struct{})
	Shutdown()
	WaitForCacheSync(stopCh <-chan struct{}) map[reflect.Type]bool
	ForResource(resource schema.GroupVersionResource) (GenericInformer, error)
	InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer
  
	Admissionregistration() admissionregistration.Interface
	Internal() apiserverinternal.Interface
	Apps() apps.Interface
	Autoscaling() autoscaling.Interface
	Batch() batch.Interface
	Certificates() certificates.Interface
	Coordination() coordination.Interface
	Core() core.Interface
	Discovery() discovery.Interface
	Events() events.Interface
	Extensions() extensions.Interface
	Flowcontrol() flowcontrol.Interface
	Networking() networking.Interface
	Node() node.Interface
	Policy() policy.Interface
	Rbac() rbac.Interface
	Resource() resource.Interface
	Scheduling() scheduling.Interface
	Storage() storage.Interface
	Storagemigration() storagemigration.Interface
}

```

### Start

`SharedInformerFactory` 下面有一个Start 方法，是负责启动所有informer的， 准确来说，是启动所有informer 中的Reflector的。

```go
func (f *sharedInformerFactory) Start(stopCh <-chan struct{}) {
	f.lock.Lock()
	defer f.lock.Unlock()
	if f.shuttingDown {
		return
	}
	for informerType, informer := range f.informers {
		if !f.startedInformers[informerType] {
			f.wg.Add(1)
			informer := informer
			go func() {
				defer f.wg.Done()
				informer.Run(stopCh)
			}()
			f.startedInformers[informerType] = true
		}
	}
}
```

### WaitForCacheSync

`WaitForCacheSync` 将会不断调用factory持有的所有informer 的 `HasSynced`方法， 直到返回 true

> HasSynced 表示全量的资源对象是否已经同步到本地存储中

```go
func (f *sharedInformerFactory) WaitForCacheSync(stopCh <-chan struct{}) map[reflect.Type]bool {
	informers := func() map[reflect.Type]cache.SharedIndexInformer {
		f.lock.Lock()
		defer f.lock.Unlock()

		informers := map[reflect.Type]cache.SharedIndexInformer{}
		for informerType, informer := range f.informers {
			if f.startedInformers[informerType] {
				informers[informerType] = informer
			}
		}
		return informers
	}()

	res := map[reflect.Type]bool{}
	for informType, informer := range informers {
    // 查看所有的informer 是否已经同步完成
		res[informType] = cache.WaitForCacheSync(stopCh, informer.HasSynced)
	}
	return res
}
```

### 使用案例

```go
package informer

import (
	selfClient "k8s-demo/client"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/informers"
	"testing"
)

func TestSharedInformer(t *testing.T) {
	clientSet := selfClient.NewClient().InitClientSet()
	factory := informers.NewSharedInformerFactory(clientSet, 0)

	// 查询pod数据, 生成PodInformer 对象
	podInformer := factory.Core().V1().Pods()

	// 生成indexer
	indexer := podInformer.Lister()

	// 启动Informer
	factory.Start(nil)

	// 等待Informer启动完成
	factory.WaitForCacheSync(nil)

	// 查询pod数据
	pods, err := indexer.List(labels.Everything())
	if err != nil {
		t.Error(err)
	}
	for _, pod := range pods {
		t.Log(pod.Namespace, pod.Name)
	}
}

```



# Controller

需求:

创建service时, 根据annotation `ingress/http: true` 判断是否自动添加ingrss

1. service创建/更新 不包含 `ingress/http: true` 不创建ingress
2. service创建/更新 包含 `ingress/http: true` 自动创建ingress
3. ingress 删除, 自动恢复
4. service 删除, 自动删除ingress

```go
// main.go
package main

import (
	"client-go-demo/pkg"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"log"
)

const KubeConfigPath = "/Users/shenweixiao/.kube/config"

func main() {
	// 1. 加载配置文件
	config, err := clientcmd.BuildConfigFromFlags("", KubeConfigPath)
	// 1.1 关闭证书验证
	config.TLSClientConfig.Insecure = true
	if err != nil {
		// 1.2 从集群中加载配置
		inClusterConfig, err := rest.InClusterConfig()
		if err != nil {
			log.Fatalln("can't get config")
		}
		config = inClusterConfig
	}
	// 2. 创建client
	clientSet := kubernetes.NewForConfigOrDie(config)
	// 3. 创建informer
	factory := informers.NewSharedInformerFactory(clientSet, 0)
	serviceInformer := factory.Core().V1().Services()
	ingressInformer := factory.Networking().V1().Ingresses()
	// 4. 创建controller
	controller := pkg.NewController(clientSet, serviceInformer, ingressInformer)

	var stopCh chan struct{}
	// 5. 启动informer
	factory.Start(stopCh)
	// 6. 等待缓存同步
	factory.WaitForCacheSync(stopCh)

	// 7. 启动controller
	controller.Run(stopCh)
}
```

```go
// pkg/controller.go

package pkg

import (
	"context"
	v16 "k8s.io/api/core/v1"
	v14 "k8s.io/api/networking/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	v15 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	v12 "k8s.io/client-go/informers/core/v1"
	v13 "k8s.io/client-go/informers/networking/v1"
	"k8s.io/client-go/kubernetes"
	coreV1 "k8s.io/client-go/listers/core/v1"
	v1 "k8s.io/client-go/listers/networking/v1"
	"k8s.io/client-go/tools/cache"
	"k8s.io/client-go/util/workqueue"
	"reflect"
	"time"
)

const workNum = 5
const maxRetry = 10

type controller struct {
	client        kubernetes.Interface
	ingressLister v1.IngressLister
	serviceLister coreV1.ServiceLister
	queue         workqueue.RateLimitingInterface
}

func (c *controller) enqueue(obj interface{}) {
	key, err := cache.MetaNamespaceKeyFunc(obj)
	if err != nil {
		runtime.HandleError(err)
	}
	c.queue.Add(key)
}

// addService 添加service
func (c *controller) addService(obj interface{}) {
	c.enqueue(obj)
}

// updateService 更新service
func (c *controller) updateService(oldObj interface{}, newObj interface{}) {
	// 比较annotation
	if reflect.DeepEqual(oldObj, newObj) {
		return
	}
	c.enqueue(newObj)
}

// deleteService 删除service
//
// 删除service时, 由于对应创建的ingress 的ownerReference 是该service
// 因此会自动触发ingress的删除事件
func (c *controller) deleteService(obj interface{}) {

}

// deleteIngress 删除ingress
func (c *controller) deleteIngress(obj interface{}) {
	ingress := obj.(*v14.Ingress)
	ownerReference := v15.GetControllerOf(ingress)
	if ownerReference == nil {
		return
	}
	if ownerReference.Kind != "Service" {
		return
	}
	//c.enqueue(obj)
	c.queue.Add(ingress.Namespace + "/" + ingress.Name)
}

// Run 启动controller
func (c *controller) Run(stopCh chan struct{}) {
	for i := 0; i < workNum; i++ {
		go wait.Until(c.worker, time.Minute, stopCh)
	}
	<-stopCh
}

func (c *controller) worker() {
	for c.processNextItem() {

	}
}

func (c *controller) processNextItem() bool {
	item, shutdown := c.queue.Get()
	if shutdown {
		return false
	}
	defer c.queue.Done(item)

	key := item.(string)
	err := c.syncService(key)
	if err != nil {
		c.handleError(key, err)
	}
	return true
}

// syncService 根据给定的服务项信息同步更新或删除对应的 Ingress 资源。
// 参数 item 表示服务项的键，格式为 "namespace/name"。
// 返回值表示同步过程中可能出现的错误。
func (c *controller) syncService(item string) error {
	namespaceKey, name, err := cache.SplitMetaNamespaceKey(item)
	if err != nil {
		return err
	}
	// 1. 获取service, 不存在则无需处理
	service, err := c.serviceLister.Services(namespaceKey).Get(name)
	if errors.IsNotFound(err) {
		return nil
	}
	// 如果是其他错误, 直接返回
	if err != nil {
		return err
	}
	// 2. 检查service 是否配置了 annotation
	_, ok := service.GetAnnotations()["ingress/http"]
	// 3. 获取相对应的 ingress
	ingress, err := c.ingressLister.Ingresses(namespaceKey).Get(name)
	// 4. 如果发生错误, 并且不是notfound, 则直接返回
	if err != nil && !errors.IsNotFound(err) {
		return err
	}
	// 5. 如果service 配置了annotation, 但是ingress不存在, 则创建ingress
	if ok && errors.IsNotFound(err) {
		// create ingress
		_ingress := c.constructIngress(service)
		_, err := c.client.NetworkingV1().Ingresses(namespaceKey).Create(context.Background(), _ingress, v15.CreateOptions{})
		if err != nil {
			return err
		}
	} else if !ok && ingress != nil {
		// 6. 如果service 没有配置annotation, 但是ingress存在, 则删除ingress
		// delete ingress
		c.client.NetworkingV1().Ingresses(namespaceKey).Delete(context.Background(), name, v15.DeleteOptions{})
	}
	return nil
}

// handleError 处理队列中的错误项。
// 当一项任务在处理时发生错误，此函数将根据错误的重试次数决定是否对该任务进行重试，并记录错误。
// key: 代表队列中特定任务的键。
// err: 任务处理过程中遇到的错误。
func (c *controller) handleError(key string, err error) {
	if c.queue.NumRequeues(key) <= maxRetry {
		c.queue.AddRateLimited(key)
	}
	runtime.HandleError(err)
	c.queue.Forget(key)
}

// constructIngress 根据Service 创建一个默认Ingress
func (c *controller) constructIngress(service *v16.Service) *v14.Ingress {
	ingress := v14.Ingress{
		TypeMeta: v15.TypeMeta{},
		ObjectMeta: v15.ObjectMeta{
			Name: service.Name,
		},
		Spec:   v14.IngressSpec{},
		Status: v14.IngressStatus{},
	}
	// 设置ownerReference
	ingress.OwnerReferences = []v15.OwnerReference{
		*v15.NewControllerRef(service, v15.SchemeGroupVersion.WithKind("Service")),
	}
	ingress.Namespace = service.Namespace
	pathType := v14.PathTypePrefix
	ingressClass := "nginx"
	ingress.Spec = v14.IngressSpec{
		IngressClassName: &ingressClass,
		Rules: []v14.IngressRule{
			{
				Host: "example.com", IngressRuleValue: v14.IngressRuleValue{
					HTTP: &v14.HTTPIngressRuleValue{
						Paths: []v14.HTTPIngressPath{
							{
								Path:     "/",
								PathType: &pathType,
								Backend: v14.IngressBackend{
									Service: &v14.IngressServiceBackend{
										Name: service.Name,
										Port: v14.ServiceBackendPort{
											Name: "http",
										},
									},
								},
							},
						},
					},
				},
			},
		},
	}
	return &ingress
}

// NewController 创建一个新的控制器实例。
//
// client: 用于与Kubernetes API进行交互的客户端接口。
// serviceInformer: service信息的缓存实例，用于监听service资源的变化。
// ingressInformer: Ingress信息的缓存实例，用于监听Ingress资源的变化。
// 返回值: 初始化后的controller实例。
func NewController(client kubernetes.Interface, serviceInformer v12.ServiceInformer, ingressInformer v13.IngressInformer) controller {
	c := controller{
		client:        client,
		ingressLister: ingressInformer.Lister(),
		serviceLister: serviceInformer.Lister(),
		// 创建一个work queue，用于存储待处理的资源对象。
		queue: workqueue.NewRateLimitingQueueWithConfig(workqueue.DefaultControllerRateLimiter(), workqueue.RateLimitingQueueConfig{
			Name: "xsw-ingress-controller",
		}),
	}
	// 为serviceInformer注册事件监听器，监听service资源的变化。
	serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    c.addService,
		UpdateFunc: c.updateService,
		DeleteFunc: c.deleteService,
	})
	// 为ingressInformer注册事件监听器，监听Ingress资源的变化。
	ingressInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: c.deleteIngress,
	})
	return c
}

```

